{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oneokiwa/DNN-project/blob/main/RobreTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD5aaycRv3r5"
      },
      "outputs": [],
      "source": [
        "!pip install nbstripout\n",
        "!nbstripout /content/your_notebook.ipynb # 깃허브에서 preview 안 보이는 문제 해결"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRPTeqdsOLV8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hVJmvFCd91s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/AI_Human.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5wGX7E-PkXA"
      },
      "outputs": [],
      "source": [
        "!cp '/content/drive/MyDrive/AI_Human.csv' \"/content/\" # 학습용 데이터셋\n",
        "!cp '/content/drive/MyDrive/daigt.csv' \"/content/\" # 테스트용 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmEyG-uMXHq3"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/drive/MyDrive/final-tunned-roberta-ai-vs-human' \"/content/\" # strong decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkD9p7VP2wP"
      },
      "source": [
        "### 📦 Install / Imports & helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOxf0Ug1P0p_"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,4,5,6,7\"\n",
        "\n",
        "import torch\n",
        "\n",
        "# GPU 사용 가능한지 확인\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"사용 중인 GPU 이름:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU를 사용할 수 없습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2UyNLWdP5iH"
      },
      "outputs": [],
      "source": [
        "import os, datetime as dt, json, random, numpy as np, pandas as pd, torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    RobertaTokenizerFast, RobertaForSequenceClassification,\n",
        "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback, TrainerCallback,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import disable_caching\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmID4ETIP8Pc"
      },
      "source": [
        "### 하이퍼파라미터 튜닝을 위한 조합 실험 (1) 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWQz2kGBP67X"
      },
      "outputs": [],
      "source": [
        "RAW_PATH = \"/content/AI_Human.csv\"\n",
        "assert os.path.exists(RAW_PATH), f\"{RAW_PATH} not found!\"\n",
        "\n",
        "df_raw = (\n",
        "    pd.read_csv(RAW_PATH, usecols=[\"Generation\", \"label\"])\n",
        "      .dropna(subset=[\"Generation\"])\n",
        "      .rename(columns={\"Generation\": \"text\"})\n",
        ")\n",
        "\n",
        "df_raw[\"text_norm\"] = df_raw[\"text\"].str.lower().str.strip()\n",
        "df_raw[\"label\"] = df_raw[\"label\"].astype(int)\n",
        "\n",
        "before, after = len(df_raw), df_raw[\"text_norm\"].nunique()\n",
        "df_raw = df_raw.drop_duplicates(subset=\"text_norm\")\n",
        "print(f\"Removed {before - after:,} exact duplicate rows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnNCAxAPW1up"
      },
      "source": [
        "### 하이퍼파라미터 튜닝을 위한 조합 실험 (2) 실험 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAS95n0CW4pH"
      },
      "outputs": [],
      "source": [
        "# 1) 샘플링 (전체 10%)\n",
        "SEED = 42\n",
        "df_small = df_raw.sample(frac=0.10, random_state=SEED)\n",
        "\n",
        "# 2) train/val 분할 (8:2)\n",
        "train_df_small, val_df_small = train_test_split(\n",
        "    df_small,\n",
        "    test_size=0.2,\n",
        "    stratify=df_small[\"label\"],\n",
        "    random_state=SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBG_Sp85XH4v"
      },
      "outputs": [],
      "source": [
        "# 3) Tokenizer 설정\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "MAX_LEN = 512\n",
        "HEAD    = 256\n",
        "TAIL    = 254\n",
        "\n",
        "def head_tail_tokenize(batch):\n",
        "    encodings = {\"input_ids\": [], \"attention_mask\": []}\n",
        "    for text in batch[\"text\"]:\n",
        "        ids = tokenizer.encode(text, add_special_tokens=True, truncation=False)\n",
        "        if len(ids) > MAX_LEN:\n",
        "            ids = ids[: HEAD + 1] + ids[-TAIL:]\n",
        "            ids = ids[:MAX_LEN]\n",
        "        attn = [1] * len(ids)\n",
        "        encodings[\"input_ids\"].append(ids)\n",
        "        encodings[\"attention_mask\"].append(attn)\n",
        "    return encodings\n",
        "\n",
        "# 4) Dataset 변환 및 토크나이징\n",
        "disable_caching()\n",
        "\n",
        "train_ds_small = Dataset.from_pandas(train_df_small[[\"text\", \"label\"]]).map(\n",
        "    head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc=1\n",
        ")\n",
        "val_ds_small = Dataset.from_pandas(val_df_small[[\"text\", \"label\"]]).map(\n",
        "    head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc=1\n",
        ")\n",
        "\n",
        "# 5) Collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d6eXnmVpE6a"
      },
      "outputs": [],
      "source": [
        "# 튜닝 자동화\n",
        "# 튜닝 대상 조합 정의\n",
        "configs = [\n",
        "    {\"name\": \"baseline\",       \"lr\": 2e-5, \"wd\": 0.01},\n",
        "    {\"name\": \"high_lr\",        \"lr\": 3e-5, \"wd\": 0.01},\n",
        "    {\"name\": \"strong_decay\",   \"lr\": 2e-5, \"wd\": 0.1},\n",
        "]\n",
        "\n",
        "# 성능 지표\n",
        "def compute_metrics(pred):\n",
        "    y_true = pred.label_ids\n",
        "    y_pred = pred.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\":  accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred),\n",
        "        \"recall\":    recall_score(y_true, y_pred),\n",
        "        \"f1\":        f1_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "# 로그 콜백\n",
        "class LogCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            now = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
        "            # 안정적으로 logs에서 가져오기\n",
        "            lr = logs.get(\"learning_rate\", 0.0)\n",
        "            watched = {\n",
        "                \"loss\": logs.get(\"loss\"),\n",
        "                \"eval_loss\": logs.get(\"eval_loss\"),\n",
        "                \"eval_accuracy\": logs.get(\"eval_accuracy\"),\n",
        "                \"eval_f1\": logs.get(\"eval_f1\"),\n",
        "                \"lr\": lr,\n",
        "            }\n",
        "            # msg = \" | \".join(f\"{k}: {v:.4f}\" for k,v in watched.items() if v is not None)\n",
        "            msg = \" | \".join(f\"{k}: {v:.6f}\" if k == \"lr\" else f\"{k}: {v:.4f}\" for k,v in watched.items() if v is not None)\n",
        "            print(f\"[{now}] step {state.global_step} | {msg}\")\n",
        "\n",
        "\n",
        "# 실험 반복\n",
        "for cfg in configs:\n",
        "    print(f\"\\n🚀 실험 시작: {cfg['name']}\")\n",
        "\n",
        "    # 고유 로그/모델 저장 경로\n",
        "    timestamp = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_id = f\"{cfg['name']}_lr{cfg['lr']}_wd{cfg['wd']}_{timestamp}\"\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\",\n",
        "        num_labels=2,\n",
        "        hidden_dropout_prob=0.2,\n",
        "        attention_probs_dropout_prob=0.2,\n",
        "    ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # 학습 설정\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"/content/tune_outputs/{run_id}\",\n",
        "        per_device_train_batch_size=56,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=cfg['lr'],\n",
        "        warmup_ratio=0.1,           # 전체 학습 스텝 중 10%를 warmup\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        weight_decay=cfg['wd'],\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        label_smoothing_factor=0.1,\n",
        "        eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        logging_dir=f\"/content/logs/{run_id}\",\n",
        "        logging_steps=10, logging_first_step=True,\n",
        "        save_total_limit=1,\n",
        "        run_name=run_id,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    # Trainer 구성\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds_small,\n",
        "        eval_dataset=val_ds_small,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2), LogCallback()],\n",
        "    )\n",
        "\n",
        "    # 학습 실행\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnv3Ir6Yypvv"
      },
      "source": [
        "### 🧹 Load & clean raw data (dedup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbckqyrQynlK"
      },
      "outputs": [],
      "source": [
        "RAW_PATH1 = \"/content/AI_Human.csv\"\n",
        "assert os.path.exists(RAW_PATH1), f\"{RAW_PATH1} not found!\"\n",
        "\n",
        "df_raw1 = (\n",
        "    pd.read_csv(RAW_PATH1, usecols=[\"Generation\", \"label\"])\n",
        "      .dropna(subset=[\"Generation\"])\n",
        "      .rename(columns={\"Generation\": \"text\"})\n",
        ")\n",
        "\n",
        "df_raw1[\"text_norm\"] = df_raw1[\"text\"].str.lower().str.strip()\n",
        "df_raw1[\"label\"] = df_raw1[\"label\"].astype(int)\n",
        "\n",
        "before, after = len(df_raw1), df_raw1[\"text_norm\"].nunique()\n",
        "df_raw1 = df_raw1.drop_duplicates(subset=\"text_norm\")\n",
        "print(f\"Removed {before - after:,} exact duplicate rows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSM-ZIj3P-0l"
      },
      "source": [
        "### ✂️ Split or load cached splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzb6KizRP_u0"
      },
      "outputs": [],
      "source": [
        "CACHE_DIR = \"splits_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "paths = {name: f\"{CACHE_DIR}/{name}.parquet\" for name in [\"train\", \"val\", \"test\"]}\n",
        "\n",
        "if all(os.path.exists(p) for p in paths.values()):\n",
        "    print(\"📂 Cached splits found – loading.\")\n",
        "    train_df = pd.read_parquet(paths[\"train\"])\n",
        "    val_df   = pd.read_parquet(paths[\"val\"])\n",
        "    test_df  = pd.read_parquet(paths[\"test\"])\n",
        "else:\n",
        "    print(\"⚙️  Creating new splits.\")\n",
        "    gss1 = GroupShuffleSplit(train_size=0.8, random_state=SEED, n_splits=1)\n",
        "    train_idx, temp_idx = next(gss1.split(df_raw1, groups=df_raw1[\"text_norm\"]))\n",
        "    train_df = df_raw1.iloc[train_idx]\n",
        "    temp_df  = df_raw1.iloc[temp_idx]\n",
        "\n",
        "    gss2 = GroupShuffleSplit(train_size=0.5, random_state=SEED, n_splits=1)\n",
        "    val_idx, test_idx = next(gss2.split(temp_df, groups=temp_df[\"text_norm\"]))\n",
        "    val_df  = temp_df.iloc[val_idx]\n",
        "    test_df = temp_df.iloc[test_idx]\n",
        "\n",
        "    train_df.to_parquet(paths[\"train\"])\n",
        "    val_df.to_parquet(paths[\"val\"])\n",
        "    test_df.to_parquet(paths[\"test\"])\n",
        "    print(\"Splits saved to 'splits_cache/'.\")\n",
        "\n",
        "overlap = set(train_df[\"text_norm\"]) & set(val_df[\"text_norm\"])\n",
        "print(\"train ∩ val duplicates:\", len(overlap))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlGUWLXBmd04"
      },
      "source": [
        "### - ✔️ 5-fold cross validation (t-test 하기 위해)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3LIrSnTmhJW"
      },
      "outputs": [],
      "source": [
        "# 설정\n",
        "N_SPLITS = 5\n",
        "CACHE_DIR = \"cv_splits_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# 전체 데이터\n",
        "df = df_raw1.copy()\n",
        "groups = df[\"text_norm\"]\n",
        "\n",
        "# 캐시 여부 확인\n",
        "fold_paths = [f\"{CACHE_DIR}/fold_{i}.parquet\" for i in range(N_SPLITS)]\n",
        "if all(os.path.exists(p) for p in fold_paths):\n",
        "    print(\"📂 Cached CV folds found – loading.\")\n",
        "    folds = [pd.read_parquet(p) for p in fold_paths]\n",
        "else:\n",
        "    print(\"⚙️  Creating new 5-Fold CV splits.\")\n",
        "    gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "    folds = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(df, groups=groups)):\n",
        "        df_fold = df.copy().reset_index(drop=True)\n",
        "        df_fold[\"fold\"] = -1\n",
        "        df_fold.loc[train_idx, \"fold\"] = 0  # train: 0\n",
        "        df_fold.loc[val_idx,   \"fold\"] = 1  # val: 1\n",
        "        df_fold.to_parquet(fold_paths[fold_idx], index=False)\n",
        "        folds.append(df_fold)\n",
        "\n",
        "    print(f\"Saved {N_SPLITS} folds to '{CACHE_DIR}/'.\")\n",
        "\n",
        "# 예시: 0번 fold에서 train/val 분리\n",
        "df_fold0 = folds[0]\n",
        "train_df = df_fold0[df_fold0[\"fold\"] == 0].drop(columns=\"fold\").reset_index(drop=True)\n",
        "val_df   = df_fold0[df_fold0[\"fold\"] == 1].drop(columns=\"fold\").reset_index(drop=True)\n",
        "\n",
        "print(f\"[Fold 0] train: {len(train_df)}, val: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLvKjv0wne0G"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return dict(\n",
        "        accuracy  = accuracy_score(labels, preds),\n",
        "        precision = precision_score(labels, preds),\n",
        "        recall    = recall_score(labels, preds),\n",
        "        f1        = f1_score(labels, preds),\n",
        "    )\n",
        "\n",
        "all_results = []  # 평가 지표 저장용 리스트\n",
        "\n",
        "class MinimalLogCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics:\n",
        "            print(f\"[step {state.global_step}] F1 = {metrics.get('eval_f1', -1):.4f}\")\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n📂 Fold {i+1}/5\")\n",
        "\n",
        "    df_fold = folds[i]\n",
        "    train_df = df_fold[df_fold[\"fold\"] == 0].drop(columns=\"fold\").reset_index(drop=True)\n",
        "    val_df   = df_fold[df_fold[\"fold\"] == 1].drop(columns=\"fold\").reset_index(drop=True)\n",
        "\n",
        "    # Tokenize\n",
        "    train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]]).map(\n",
        "        head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc=1\n",
        "    )\n",
        "    val_ds = Dataset.from_pandas(val_df[[\"text\", \"label\"]]).map(\n",
        "        head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc=1\n",
        "    )\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2,\n",
        "        hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2,\n",
        "    ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # Trainer 정의\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"/content/decay-roberta-fold{i+1}\",\n",
        "        per_device_train_batch_size=56,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=2e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        weight_decay=0.1,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        label_smoothing_factor=0.1,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"no\",\n",
        "        report_to=[],\n",
        "        disable_tqdm=False,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=training_args,\n",
        "        train_dataset=train_ds, eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer, data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[MinimalLogCallback()],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # 평가\n",
        "    result = trainer.evaluate()\n",
        "    print(f\"Fold {i+1} metrics: {result}\")\n",
        "\n",
        "    all_results.append({\n",
        "        \"fold\": i+1,\n",
        "        \"accuracy\":  result[\"eval_accuracy\"],\n",
        "        \"precision\": result[\"eval_precision\"],\n",
        "        \"recall\":    result[\"eval_recall\"],\n",
        "        \"f1\":        result[\"eval_f1\"]\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UaE9DWYn0mk"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(all_results) # t-test에 바로 사용\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuD-FycyAnJH"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(all_results).to_csv(\"/content/drive/MyDrive/decay_cv_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_O_xLCzDP5W"
      },
      "outputs": [],
      "source": [
        "# memo\n",
        "# 연결 끊어진 경우,\n",
        "# folds 불러오기\n",
        "fold_paths = [f\"/content/cv_splits_cache/fold_{i}.parquet\" for i in range(5)]\n",
        "folds = [pd.read_parquet(p) for p in fold_paths]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMeqjRx7AC04"
      },
      "outputs": [],
      "source": [
        "# ======= baseline 5-fold ======= #\n",
        "\n",
        "baseline_all_results = []  # 평가 지표 저장용 리스트\n",
        "\n",
        "# for i in [3, 4]:  # fold 4, 5만 실행 (index는 0부터 시작)\n",
        "for i in range(5):\n",
        "    print(f\"\\n📂 Fold {i+1}/5\")\n",
        "\n",
        "    df_fold = folds[i]\n",
        "    train_df = df_fold[df_fold[\"fold\"] == 0].drop(columns=\"fold\").reset_index(drop=True)\n",
        "    val_df   = df_fold[df_fold[\"fold\"] == 1].drop(columns=\"fold\").reset_index(drop=True)\n",
        "\n",
        "    # Tokenize\n",
        "    train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]]).map(\n",
        "        head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc=1\n",
        "    )\n",
        "    val_ds = Dataset.from_pandas(val_df[[\"text\", \"label\"]]).map(\n",
        "        head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc=1\n",
        "    )\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2,\n",
        "        hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2,\n",
        "    ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # Trainer 정의\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"/content/baseline-roberta-fold{i+1}\",\n",
        "        per_device_train_batch_size=56,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=2e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        weight_decay=0.01,                # baseline\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        label_smoothing_factor=0.1,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"no\",\n",
        "        report_to=[],\n",
        "        disable_tqdm=False,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=training_args,\n",
        "        train_dataset=train_ds, eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer, data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[MinimalLogCallback()],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # 평가\n",
        "    result = trainer.evaluate()\n",
        "    print(f\"Fold {i+1} metrics: {result}\")\n",
        "\n",
        "    baseline_all_results.append({\n",
        "        \"fold\": i+1,\n",
        "        \"accuracy\":  result[\"eval_accuracy\"],\n",
        "        \"precision\": result[\"eval_precision\"],\n",
        "        \"recall\":    result[\"eval_recall\"],\n",
        "        \"f1\":        result[\"eval_f1\"]\n",
        "    })\n",
        "\n",
        "    pd.DataFrame(baseline_all_results).to_csv(\"/content/drive/MyDrive/baseline_results_partial.csv\", index=False) # 연결 끊어짐 대비 중간 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ3AhRzZC24a"
      },
      "outputs": [],
      "source": [
        "baseline_all_results = pd.read_csv(\"/content/drive/MyDrive/decay_cv_results.csv\").to_dict(\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbe8q4BiD_Hj"
      },
      "outputs": [],
      "source": [
        "import shutil # 연결 끊어짐 대비 fold 캐시 저장\n",
        "\n",
        "src_dir = \"/content/cv_splits_cache\"\n",
        "dst_dir = \"/content/drive/MyDrive/cv_splits_cache\"\n",
        "\n",
        "shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n",
        "print(\"복사 완료:\", dst_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv2csZGkZDKK"
      },
      "source": [
        "### - ☑️ T-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS-UYJxnaSuY"
      },
      "outputs": [],
      "source": [
        "acc_bl = np.array([0.9824972837, 0.9728476607, 0.9847669865, 0.9619174663, 0.9766448642])\n",
        "acc_de = np.array([0.9721484127, 0.9627892466, 0.975396962, 0.9601101596, 0.9776560954])\n",
        "\n",
        "t_stat, p_val = ttest_rel(acc_de, acc_bl)\n",
        "\n",
        "print(\"baseline: \", np.mean(acc_bl))\n",
        "print(\"decay: \", np.mean(acc_de))\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnCHyiMQQCQ2"
      },
      "source": [
        "### 🔠 Tokenize & build HF Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XjxzZGWQDIW"
      },
      "outputs": [],
      "source": [
        "tok = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "ds = Dataset.from_pandas(df_raw1[[\"text\"]], preserve_index=False)\n",
        "\n",
        "def add_len(batch):\n",
        "    batch[\"tok_len\"] = [len(t) for t in tok(batch[\"text\"], add_special_tokens=True)[\"input_ids\"]]\n",
        "    return batch\n",
        "\n",
        "disable_caching()\n",
        "\n",
        "ds = ds.map(add_len, batched=True, batch_size=1024, num_proc=1, desc=\"Adding token lengths\")\n",
        "lengths = ds[\"tok_len\"]\n",
        "\n",
        "pct = np.percentile(lengths, [50, 90, 95, 99])\n",
        "print(\"median / p90 / p95 / p99 =\", pct)\n",
        "print(\"max =\", max(lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VawV3mj8QEzQ"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "MAX_LEN = 512\n",
        "HEAD    = 256\n",
        "TAIL    = 254\n",
        "\n",
        "def head_tail_tokenize(batch):\n",
        "    encodings = {\"input_ids\": [], \"attention_mask\": []}\n",
        "    for text in batch[\"text\"]:\n",
        "        ids = tokenizer.encode(text, add_special_tokens=True, truncation=False)\n",
        "        if len(ids) > MAX_LEN:\n",
        "            # ids[0] : <s>, ids[-1] : </s>\n",
        "            new_ids = ids[: HEAD + 1] + ids[-TAIL:]\n",
        "            ids = new_ids[:MAX_LEN]\n",
        "        attn = [1] * len(ids)\n",
        "        encodings[\"input_ids\"].append(ids)\n",
        "        encodings[\"attention_mask\"].append(attn)\n",
        "    return encodings\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]]).map(\n",
        "    head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc = 1\n",
        ")\n",
        "val_ds = Dataset.from_pandas(val_df[[\"text\", \"label\"]]).map(\n",
        "    head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc = 1\n",
        ")\n",
        "test_ds = Dataset.from_pandas(test_df[[\"text\", \"label\"]]).map(\n",
        "    head_tail_tokenize, batched=True, remove_columns=[\"text\"], num_proc = 1\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ2SahY6QGuL"
      },
      "source": [
        "### 🏗️ Build model (RoBERTa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQSwO76hQHdF"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", num_labels=2,\n",
        "    hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYmTDUj5QIcq"
      },
      "source": [
        "### ⚙️ TrainingArguments (튜닝된 하이퍼파라미터)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5KQG-aVQJWg"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/roberta-output\",\n",
        "    per_device_train_batch_size=56,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,           # 3 -> 2\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    weight_decay=0.01,             # 0.01 -> 0.1 (t-test 기각으로 기본값 유지)\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    label_smoothing_factor=0.1,\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"f1\",\n",
        "    logging_dir=\"./logs\", logging_steps=10, logging_first_step=True,\n",
        "    save_total_limit=1, run_name=\"roberta-ai-vs-human\", report_to=[\"tensorboard\"],\n",
        "    ddp_find_unused_parameters=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bgcyGIVQKqT"
      },
      "source": [
        "### 🚂 Trainer & train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5TuYER6QLeY"
      },
      "outputs": [],
      "source": [
        "# 로그 콜백 수정\n",
        "class LogCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            now = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
        "            # 안정적으로 logs에서 가져오기\n",
        "            lr = logs.get(\"learning_rate\", 0.0)\n",
        "            watched = {\n",
        "                \"loss\": logs.get(\"loss\"),\n",
        "                \"eval_loss\": logs.get(\"eval_loss\"),\n",
        "                \"eval_accuracy\": logs.get(\"eval_accuracy\"),\n",
        "                \"eval_f1\": logs.get(\"eval_f1\"),\n",
        "                \"lr\": lr,\n",
        "            }\n",
        "            msg = \" | \".join(f\"{k}: {v:.6f}\" if k == \"lr\" else f\"{k}: {v:.4f}\" for k,v in watched.items() if v is not None)\n",
        "            print(f\"[{now}] step {state.global_step} | {msg}\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return dict(\n",
        "        accuracy  = accuracy_score(labels, preds),\n",
        "        precision = precision_score(labels, preds),\n",
        "        recall    = recall_score(labels, preds),\n",
        "        f1        = f1_score(labels, preds),\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args,\n",
        "    train_dataset=train_ds, eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer, data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(2), LogCallback()],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJE5UpZsQNYD"
      },
      "source": [
        "### 🧪 Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uBDorFWQOFD"
      },
      "outputs": [],
      "source": [
        "print(\"📊 Test metrics:\", trainer.evaluate(test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdwGVtm-QPR_"
      },
      "outputs": [],
      "source": [
        "# 1) 모델/토크나이저 불러오기\n",
        "MODEL_DIR = \"/content/final-baseline-tunned-roberta-ai-vs-human\"\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_DIR)\n",
        "model     = RobertaForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# 2) DAIGT 데이터 로드 & 전처리\n",
        "DAIGT_PATH = \"/content/daigt.csv\"\n",
        "df = (pd.read_csv(DAIGT_PATH, usecols=[\"text\", \"generated\"])\n",
        "        .rename(columns={\"generated\":\"label\"})\n",
        "        .dropna(subset=[\"text\"]))\n",
        "df[\"text\"]  = df[\"text\"].astype(str).str.strip()\n",
        "df[\"label\"] = df[\"label\"].astype(int)\n",
        "\n",
        "# 3) Head-Tail 토크나이즈\n",
        "MAX_LEN, HEAD, TAIL = 512, 256, 254\n",
        "def ht_tokenize(batch):\n",
        "    ids_all, attn_all = [], []\n",
        "    for txt in batch[\"text\"]:\n",
        "        ids = tokenizer.encode(txt, add_special_tokens=True, truncation=False)\n",
        "        if len(ids) > MAX_LEN:\n",
        "            ids = ids[:HEAD+1] + ids[-TAIL:]\n",
        "        ids_all.append(ids)\n",
        "        attn_all.append([1]*len(ids))\n",
        "    return {\"input_ids\": ids_all, \"attention_mask\": attn_all}\n",
        "\n",
        "ds_test = Dataset.from_pandas(df[[\"text\",\"label\"]]).map(\n",
        "    ht_tokenize, batched=True, batch_size=1024,\n",
        "    num_proc=20, remove_columns=[\"text\"], desc=\"Tokenizing(DAIGT)\"\n",
        ")\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# 4) 테스트용 Trainer\n",
        "def metrics(p):\n",
        "    y, pred = p.label_ids, p.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\":  accuracy_score(y, pred),\n",
        "        \"precision\": precision_score(y, pred),\n",
        "        \"recall\":    recall_score(y, pred),\n",
        "        \"f1\":        f1_score(y, pred),\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"tmp-eval\",\n",
        "        per_device_eval_batch_size=32,\n",
        "        dataloader_drop_last=False,\n",
        "        seed=42,\n",
        "    ),\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=metrics,\n",
        ")\n",
        "\n",
        "# 5) 테스트 실행\n",
        "results = trainer.evaluate(ds_test)\n",
        "print(\"\\n📊 DAIGT Test metrics\")\n",
        "for k,v in results.items():\n",
        "    if k.startswith(\"eval_\"):\n",
        "        print(f\"{k:12s}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7BzjmfTirpv"
      },
      "outputs": [],
      "source": [
        "# 📊 DAIGT Test metrics (strong decay)\n",
        "# eval_loss   : 0.0752\n",
        "# eval_model_preparation_time: 0.0033\n",
        "# eval_accuracy: 0.9861\n",
        "# eval_precision: 0.9743\n",
        "# eval_recall : 0.9985\n",
        "# eval_f1     : 0.9863\n",
        "# eval_runtime: 4.3530\n",
        "# eval_samples_per_second: 627.1530\n",
        "# eval_steps_per_second: 19.7560\n",
        "\n",
        "# ==================================== #\n",
        "\n",
        "# 📊 DAIGT Test metrics (baseline)\n",
        "# eval_loss   : 0.0785\n",
        "# eval_model_preparation_time: 0.0033\n",
        "# eval_accuracy: 0.9839\n",
        "# eval_precision: 0.9708\n",
        "# eval_recall : 0.9978\n",
        "# eval_f1     : 0.9841\n",
        "# eval_runtime: 4.3121\n",
        "# eval_samples_per_second: 633.0960\n",
        "# eval_steps_per_second: 19.9440"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5fneY1MQVI6"
      },
      "source": [
        "### 💾 Save model/tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_R5jPPqQR-I"
      },
      "outputs": [],
      "source": [
        "# SAVE_PATH = \"/content/drive/MyDrive/final-baseline-tunned-roberta-ai-vs-human\"\n",
        "SAVE_PATH = \"/content/final-baseline-tunned-roberta-ai-vs-human\"\n",
        "\n",
        "trainer.save_model(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(f\"Model & tokenizer saved to '{SAVE_PATH}'.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVYpu5AXiZvRRkDDWl8epb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}